{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KcBERT_nsmc_binary_classification.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"i45d7E0L8bZ_"},"source":["<br>\n","<br>\n","\n","# **준비 사항**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73sTVHXAc-FP","executionInfo":{"status":"ok","timestamp":1608427194367,"user_tz":-540,"elapsed":799,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"4d2e8491-1982-47ab-844c-134240a487d6"},"source":["from google.colab import drive\n","drive.mount('content/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at content/; to attempt to forcibly remount, call drive.mount(\"content/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WkAHQrj2Vjbl"},"source":["# Hugging Face의 트랜스포머 모델을 설치\n","!pip install transformers --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"75dIz2fNWG8F"},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import BertForSequenceClassification, BertTokenizer\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime\n","import os\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_U3uMySBCIV"},"source":["<br>\n","<br>\n","\n","# **데이터 로드**"]},{"cell_type":"code","metadata":{"id":"ImBtAkSyTW1r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427199830,"user_tz":-540,"elapsed":6251,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"40f0f6b7-5c56-4013-f6e4-006113638141"},"source":["# 네이버 영화리뷰 감정분석 데이터 다운로드\n","!git clone https://github.com/e9t/nsmc.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'nsmc' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nCKSDHcXTiKn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427199830,"user_tz":-540,"elapsed":6247,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"85ae12cc-581b-42a7-8a2c-9a8a4a0081cd"},"source":["# 디렉토리의 파일 목록\n","os.listdir('nsmc/')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['code',\n"," '.git',\n"," 'ratings.txt',\n"," 'README.md',\n"," 'ratings_test.txt',\n"," 'synopses.json',\n"," 'ratings_train.txt',\n"," 'raw']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"0LPEdb2tWfIU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427200298,"user_tz":-540,"elapsed":6711,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"17f2202a-0b81-4eea-f8cc-933b85ae5811"},"source":["filepath = 'nsmc'\n","train = pd.read_table(os.path.join(filepath, 'ratings_train.txt'))\n","test = pd.read_table(os.path.join(filepath, 'ratings_test.txt'))\n","\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(150000, 3)\n","(50000, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vMvYtp6dfZsX"},"source":["# 필요 패키지 다운로드\n","!pip install soynlp --quiet  # 반복 단어 축약 soynlp repeat_normalize() 모듈 활용 예정"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZoxnix-xDY2"},"source":["# 훈련 데이터셋 전처리\n","train = train.dropna(how='any')  # nan 열 제거\n","train['document'] = train['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣!.~;^ ]', '')  # 한국어+공백 외 제거\n","train['document'].replace('', np.nan, inplace=True)  # regex에 의해 공백으로 치환된 값 nan 값으로 변경\n","train = train.dropna(how='any')  # nan 열 제거\n","\n","# 테스트 데이터셋 전처리\n","test = test.dropna(how='any')  # nan 열 제거\n","test['document'] = test['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣!.~;^ ]', '')  # 한국어+공백 외 제거\n","test['document'].replace('', np.nan, inplace=True)  # regex에 의해 공백으로 치환된 값 nan 값으로 변경\n","test = test.dropna(how='any')  # nan 열 제거\n","\n","# 반복 단어 축약\n","from soynlp.normalizer import repeat_normalize\n","\n","train['document'] = train['document'].apply(lambda sentence: repeat_normalize(sentence, num_repeats=2))\n","test['document'] = test['document'].apply(lambda sentence: repeat_normalize(sentence, num_repeats=2))\n","\n","# 중복 데이터 제거\n","train.drop_duplicates(subset=['document'], inplace=True)\n","test.drop_duplicates(subset=['document'], inplace=True)\n","\n","# 파일 저장\n","train.to_csv('nsmc_preprocessed_train', encoding='utf-8')\n","test.to_csv('nsmc_preprocessed_test', encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8TGkwcd3wifh","executionInfo":{"status":"ok","timestamp":1608427204670,"user_tz":-540,"elapsed":11072,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"941db595-ab79-4dff-fe9d-aacd2d14715c"},"source":["print(train.columns)\n","print(test.columns)\n","print(train.head())\n","print(test.head())\n","print(train.isnull().sum())\n","print(test.isnull().sum())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['id', 'document', 'label'], dtype='object')\n","Index(['id', 'document', 'label'], dtype='object')\n","         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n","        id                                     document  label\n","0  6270596                                          굳 ㅋ      1\n","2  8544678         뭐야 이 평점들은.... 나쁘진 않지만 점 짜리는 더더욱 아니잖아      0\n","3  6825595             지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n","4  6723715  만 아니었어도 별 다섯 개 줬을텐데.. 왜 로 나와서 제 심기를 불편하게 하죠      0\n","5  7898805                            음악이 주가 된 최고의 음악영화      1\n","id          0\n","document    0\n","label       0\n","dtype: int64\n","id          0\n","document    0\n","label       0\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8KkJZvhccRUJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427204671,"user_tz":-540,"elapsed":11068,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"afa1c517-db64-451f-a05b-27b84fca6363"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = train['document'].values\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 아 더빙.. 진짜 짜증나네요 목소리 [SEP]',\n"," '[CLS] 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 [SEP]',\n"," '[CLS] 너무재밓었다그래서보는것을추천한다 [SEP]',\n"," '[CLS] 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 [SEP]',\n"," '[CLS] 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다 [SEP]',\n"," '[CLS] 막 걸음마 뗀 세부터 초등학교 학년생인 살용영화.ㅋㅋㅋ...별반개도 아까움. [SEP]',\n"," '[CLS] 원작의 긴장감을 제대로 살려내지못했다. [SEP]',\n"," '[CLS] 별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네 [SEP]',\n"," '[CLS] 액션이 없는데도 재미 있는 몇안되는 영화 [SEP]',\n"," '[CLS] 왜케 평점이 낮은건데 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나 [SEP]']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"7hBblIVQcXJR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427204671,"user_tz":-540,"elapsed":11064,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"15162dc7-eaab-4c00-9e79-a2bfba82965b"},"source":["# 라벨 추출\n","labels = train['label'].values\n","labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 0, ..., 0, 1, 0])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"PwEplfDvcnZG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427224874,"user_tz":-540,"elapsed":31263,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"63d29028-559a-47c2-c2ae-c919e6244459"},"source":["# koELECTRA의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-large')\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print(sentences[0])\n","print(tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS] 아 더빙.. 진짜 짜증나네요 목소리 [SEP]\n","['[CLS]', '아', '더', '##빙', '.', '.', '진짜', '짜증나네', '##요', '목소리', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HquX9FwHj2os","executionInfo":{"status":"ok","timestamp":1608427224875,"user_tz":-540,"elapsed":31259,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"fb6578c2-3bb4-4ed4-a0fd-9b7a7e094ca5"},"source":["# 문장 최대 길이 도출\n","lengths = []\n","for token in tokenized_texts:\n","    lengths.append(len(token))\n","print(max(lengths))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["142\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VJ76KiP_dLn-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427227583,"user_tz":-540,"elapsed":33963,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"2fc65ec7-6da0-425e-ee89-8054a3784b0e"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 150\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","print(input_ids[0])\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","print(input_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3]\n","[    2  2170   832  5045    17    17  7992 29734  4040 10720     3     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pKfL8SotdVaW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427240100,"user_tz":-540,"elapsed":46476,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"45c96b52-9a2d-4d6f-8fde-1ee791955a77"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1f5Vq3-7eNKH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427241694,"user_tz":-540,"elapsed":48066,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"516a0bb5-138a-4cc2-f6df-a5c346637f28"},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, \n","                                                                                    random_state=2018, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n","                                                       input_ids,\n","                                                       random_state=2018, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\t\t\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])\n","print(train_inputs.shape)  # train 데이터 개수\n","print(validation_inputs.shape)  # validation 데이터 개수"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([    2,   393, 10840,  4435,  4530,  9340, 17196,  4029, 16174, 10073,\n","         9781, 12275,  4014,  8278, 16192,  4406,  4032, 12890,  4314,  8274,\n","        10390,  5051,  8517,  8039,  4353,  4231, 20214, 12160,  8166,  8278,\n","         8819,  4161,   217,  8517,  8039,    17,    17,    17, 10631,  8678,\n","         8529,   303,  4281, 11691, 21599,  4038, 21772,  4049, 11428, 12275,\n","         4014,  8278,  4678,  4471,  7967, 18570,  4008, 11713,  9525,  4008,\n","        11110,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0.])\n","tensor([    2,  1294,  4448, 11956,  9841, 14954, 18489,  4866,    17,     3,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0.])\n","torch.Size([130672, 150])\n","torch.Size([14520, 150])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I3vlyUJuVRo5"},"source":["# 배치 사이즈\n","batch_size = 16\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkqUHx51dffp"},"source":["<br>\n","<br>\n","\n","# **전처리 - 테스트셋**"]},{"cell_type":"code","metadata":{"id":"xgrsNuArd4pj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427241694,"user_tz":-540,"elapsed":48059,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"667d5435-fef5-486b-ef9e-364ba3e2d969"},"source":["# 리뷰 문장 추출\n","sentences = test['document']\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                                   굳 ㅋ\n","2                  뭐야 이 평점들은.... 나쁘진 않지만 점 짜리는 더더욱 아니잖아\n","3                      지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n","4           만 아니었어도 별 다섯 개 줬을텐데.. 왜 로 나와서 제 심기를 불편하게 하죠\n","5                                     음악이 주가 된 최고의 음악영화\n","6                                               진정한 쓰레기\n","7               마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가고개를 젖게한다\n","8     갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한남...\n","9             이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 모든 사람이 그렇지는 않네..\n","10                                     괜찮네요오랜만포켓몬스터잼밌어요\n","Name: document, dtype: object"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"Gtz3QZt9d4pz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427241695,"user_tz":-540,"elapsed":48056,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"da7ce71c-89ba-4097-999c-09d5ad32296a"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 굳 ㅋ [SEP]',\n"," '[CLS] 뭐야 이 평점들은.... 나쁘진 않지만 점 짜리는 더더욱 아니잖아 [SEP]',\n"," '[CLS] 지루하지는 않은데 완전 막장임... 돈주고 보기에는.... [SEP]',\n"," '[CLS] 만 아니었어도 별 다섯 개 줬을텐데.. 왜 로 나와서 제 심기를 불편하게 하죠 [SEP]',\n"," '[CLS] 음악이 주가 된 최고의 음악영화 [SEP]',\n"," '[CLS] 진정한 쓰레기 [SEP]',\n"," '[CLS] 마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가고개를 젖게한다 [SEP]',\n"," '[CLS] 갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한남무 아 그립다 동사서독같은 영화가 이건 류아류작이다 [SEP]',\n"," '[CLS] 이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 모든 사람이 그렇지는 않네.. [SEP]',\n"," '[CLS] 괜찮네요오랜만포켓몬스터잼밌어요 [SEP]']"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"li8oRajbd4p3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427241695,"user_tz":-540,"elapsed":48051,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"b639a0eb-fd7f-4e1a-e4a7-211758154424"},"source":["# 라벨 추출\n","labels = test['label'].values\n","labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, ..., 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"lvpQ49nEd4p6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427250322,"user_tz":-540,"elapsed":56667,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"daa99016-477b-4f01-de04-81ffb68eba2f"},"source":["# koELECTRA의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-large')\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print(sentences[0])\n","print(tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS] 굳 ㅋ [SEP]\n","['[CLS]', '굳', 'ㅋ', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HI9viuAvd4p_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427251171,"user_tz":-540,"elapsed":57509,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"945f561d-9557-4f8a-faa7-0c09b74921ea"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 150\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  2, 352, 192,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"v1NKmP0Fd4qD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427255768,"user_tz":-540,"elapsed":62098,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"19c13e90-3436-445a-b38f-2ef49ec62869"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RIkaYCGbd4qG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427256271,"user_tz":-540,"elapsed":62593,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"eb54fbb6-8046-409a-c74f-9a65b2ce1c2a"},"source":["# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)\n","\n","print(test_inputs[0])\n","print(test_labels[0])\n","print(test_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([  2, 352, 192,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n","tensor(1)\n","tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7gwdYv1Ad4qK"},"source":["# 배치 사이즈\n","batch_size = 16\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBvpU-Hfgcth"},"source":["<br>\n","<br>\n","\n","# **모델 생성**"]},{"cell_type":"code","metadata":{"id":"heToD1ev0mOg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427256733,"user_tz":-540,"elapsed":63045,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"7bb12362-437f-4c63-87ea-f7aa52651634"},"source":["# GPU 디바이스 이름 구함\n","device_name = tf.test.gpu_device_name()\n","\n","# GPU 디바이스 이름 검사\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f6enIxvt1FB2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427256733,"user_tz":-540,"elapsed":63037,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"458aaf73-92df-4a7b-ad44-3afe8a1b17cd"},"source":["# 디바이스 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MS2MXSiLg5zC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608427273079,"user_tz":-540,"elapsed":79376,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"efe6b5ab-17a5-4252-91e7-704f495f338f"},"source":["# 분류를 위한 BERT 모델 생성\n","model = BertForSequenceClassification.from_pretrained(\"beomi/kcbert-large\", num_labels=2)\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at beomi/kcbert-large were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30000, 1024, padding_idx=0)\n","      (position_embeddings): Embedding(300, 1024)\n","      (token_type_embeddings): Embedding(2, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"ZIdfbLTuWmxk"},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 10\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzCHV_ghj7DM"},"source":["<br>\n","<br>\n","\n","# **모델 학습**"]},{"cell_type":"code","metadata":{"id":"S0-p6pPVXCRe"},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJXISnJzCdLM"},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muU2kS2GCh4y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608467600718,"user_tz":-540,"elapsed":23380520,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"f090ddf6-5b4c-4433-8e86-d8cebca43cd3"},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 6 ========\n","Training...\n","  Batch   500  of  8,167.    Elapsed: 0:03:51.\n","  Batch 1,000  of  8,167.    Elapsed: 0:07:42.\n","  Batch 1,500  of  8,167.    Elapsed: 0:11:34.\n","  Batch 2,000  of  8,167.    Elapsed: 0:15:25.\n","  Batch 2,500  of  8,167.    Elapsed: 0:19:16.\n","  Batch 3,000  of  8,167.    Elapsed: 0:23:06.\n","  Batch 3,500  of  8,167.    Elapsed: 0:26:57.\n","  Batch 4,000  of  8,167.    Elapsed: 0:30:48.\n","  Batch 4,500  of  8,167.    Elapsed: 0:34:39.\n","  Batch 5,000  of  8,167.    Elapsed: 0:38:30.\n","  Batch 5,500  of  8,167.    Elapsed: 0:42:21.\n","  Batch 6,000  of  8,167.    Elapsed: 0:46:11.\n","  Batch 6,500  of  8,167.    Elapsed: 0:50:03.\n","  Batch 7,000  of  8,167.    Elapsed: 0:53:54.\n","  Batch 7,500  of  8,167.    Elapsed: 0:57:44.\n","  Batch 8,000  of  8,167.    Elapsed: 1:01:36.\n","\n","  Average training loss: 0.09\n","  Training epcoh took: 1:02:54\n","\n","Running Validation...\n","  Accuracy: 0.89\n","  Validation took: 0:02:03\n","\n","======== Epoch 2 / 6 ========\n","Training...\n","  Batch   500  of  8,167.    Elapsed: 0:03:51.\n","  Batch 1,000  of  8,167.    Elapsed: 0:07:41.\n","  Batch 1,500  of  8,167.    Elapsed: 0:11:32.\n","  Batch 2,000  of  8,167.    Elapsed: 0:15:23.\n","  Batch 2,500  of  8,167.    Elapsed: 0:19:14.\n","  Batch 3,000  of  8,167.    Elapsed: 0:23:05.\n","  Batch 3,500  of  8,167.    Elapsed: 0:26:56.\n","  Batch 4,000  of  8,167.    Elapsed: 0:30:47.\n","  Batch 4,500  of  8,167.    Elapsed: 0:34:38.\n","  Batch 5,000  of  8,167.    Elapsed: 0:38:30.\n","  Batch 5,500  of  8,167.    Elapsed: 0:42:21.\n","  Batch 6,000  of  8,167.    Elapsed: 0:46:12.\n","  Batch 6,500  of  8,167.    Elapsed: 0:50:03.\n","  Batch 7,000  of  8,167.    Elapsed: 0:53:53.\n","  Batch 7,500  of  8,167.    Elapsed: 0:57:44.\n","  Batch 8,000  of  8,167.    Elapsed: 1:01:35.\n","\n","  Average training loss: 0.07\n","  Training epcoh took: 1:02:52\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:02:03\n","\n","======== Epoch 3 / 6 ========\n","Training...\n","  Batch   500  of  8,167.    Elapsed: 0:03:50.\n","  Batch 1,000  of  8,167.    Elapsed: 0:07:41.\n","  Batch 1,500  of  8,167.    Elapsed: 0:11:32.\n","  Batch 2,000  of  8,167.    Elapsed: 0:15:23.\n","  Batch 2,500  of  8,167.    Elapsed: 0:19:13.\n","  Batch 3,000  of  8,167.    Elapsed: 0:23:04.\n","  Batch 3,500  of  8,167.    Elapsed: 0:26:54.\n","  Batch 4,000  of  8,167.    Elapsed: 0:30:45.\n","  Batch 4,500  of  8,167.    Elapsed: 0:34:36.\n","  Batch 5,000  of  8,167.    Elapsed: 0:38:27.\n","  Batch 5,500  of  8,167.    Elapsed: 0:42:17.\n","  Batch 6,000  of  8,167.    Elapsed: 0:46:08.\n","  Batch 6,500  of  8,167.    Elapsed: 0:49:59.\n","  Batch 7,000  of  8,167.    Elapsed: 0:53:50.\n","  Batch 7,500  of  8,167.    Elapsed: 0:57:41.\n","  Batch 8,000  of  8,167.    Elapsed: 1:01:32.\n","\n","  Average training loss: 0.06\n","  Training epcoh took: 1:02:49\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:02:03\n","\n","======== Epoch 4 / 6 ========\n","Training...\n","  Batch   500  of  8,167.    Elapsed: 0:03:51.\n","  Batch 1,000  of  8,167.    Elapsed: 0:07:43.\n","  Batch 1,500  of  8,167.    Elapsed: 0:11:34.\n","  Batch 2,000  of  8,167.    Elapsed: 0:15:26.\n","  Batch 2,500  of  8,167.    Elapsed: 0:19:18.\n","  Batch 3,000  of  8,167.    Elapsed: 0:23:09.\n","  Batch 3,500  of  8,167.    Elapsed: 0:27:01.\n","  Batch 4,000  of  8,167.    Elapsed: 0:30:53.\n","  Batch 4,500  of  8,167.    Elapsed: 0:34:44.\n","  Batch 5,000  of  8,167.    Elapsed: 0:38:35.\n","  Batch 5,500  of  8,167.    Elapsed: 0:42:26.\n","  Batch 6,000  of  8,167.    Elapsed: 0:46:18.\n","  Batch 6,500  of  8,167.    Elapsed: 0:50:09.\n","  Batch 7,000  of  8,167.    Elapsed: 0:54:01.\n","  Batch 7,500  of  8,167.    Elapsed: 0:57:52.\n","  Batch 8,000  of  8,167.    Elapsed: 1:01:43.\n","\n","  Average training loss: 0.04\n","  Training epcoh took: 1:03:00\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:02:04\n","\n","======== Epoch 5 / 6 ========\n","Training...\n","  Batch   500  of  8,167.    Elapsed: 0:03:50.\n","  Batch 1,000  of  8,167.    Elapsed: 0:07:41.\n","  Batch 1,500  of  8,167.    Elapsed: 0:11:32.\n","  Batch 2,000  of  8,167.    Elapsed: 0:15:23.\n","  Batch 2,500  of  8,167.    Elapsed: 0:19:15.\n","  Batch 3,000  of  8,167.    Elapsed: 0:23:06.\n","  Batch 3,500  of  8,167.    Elapsed: 0:26:58.\n","  Batch 4,000  of  8,167.    Elapsed: 0:30:49.\n","  Batch 4,500  of  8,167.    Elapsed: 0:34:41.\n","  Batch 5,000  of  8,167.    Elapsed: 0:38:31.\n","  Batch 5,500  of  8,167.    Elapsed: 0:42:22.\n","  Batch 6,000  of  8,167.    Elapsed: 0:46:14.\n","  Batch 6,500  of  8,167.    Elapsed: 0:50:05.\n","  Batch 7,000  of  8,167.    Elapsed: 0:53:57.\n","  Batch 7,500  of  8,167.    Elapsed: 0:57:48.\n","  Batch 8,000  of  8,167.    Elapsed: 1:01:40.\n","\n","  Average training loss: 0.03\n","  Training epcoh took: 1:02:57\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:02:04\n","\n","======== Epoch 6 / 6 ========\n","Training...\n","  Batch   500  of  8,167.    Elapsed: 0:03:50.\n","  Batch 1,000  of  8,167.    Elapsed: 0:07:40.\n","  Batch 1,500  of  8,167.    Elapsed: 0:11:30.\n","  Batch 2,000  of  8,167.    Elapsed: 0:15:21.\n","  Batch 2,500  of  8,167.    Elapsed: 0:19:12.\n","  Batch 3,000  of  8,167.    Elapsed: 0:23:02.\n","  Batch 3,500  of  8,167.    Elapsed: 0:26:53.\n","  Batch 4,000  of  8,167.    Elapsed: 0:30:43.\n","  Batch 4,500  of  8,167.    Elapsed: 0:34:34.\n","  Batch 5,000  of  8,167.    Elapsed: 0:38:24.\n","  Batch 5,500  of  8,167.    Elapsed: 0:42:15.\n","  Batch 6,000  of  8,167.    Elapsed: 0:46:06.\n","  Batch 6,500  of  8,167.    Elapsed: 0:49:57.\n","  Batch 7,000  of  8,167.    Elapsed: 0:53:47.\n","  Batch 7,500  of  8,167.    Elapsed: 0:57:38.\n","  Batch 8,000  of  8,167.    Elapsed: 1:01:28.\n","\n","  Average training loss: 0.02\n","  Training epcoh took: 1:02:45\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:02:04\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6BVbl4Zjatzn"},"source":["<br>\n","<br>\n","\n","# **테스트셋 평가**"]},{"cell_type":"code","metadata":{"id":"c5KHb6RkbHdj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608468019895,"user_tz":-540,"elapsed":419162,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"947047ff-0281-4e28-af8e-b78bcdc27905"},"source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  Batch   100  of  3,051.    Elapsed: 0:00:14.\n","  Batch   200  of  3,051.    Elapsed: 0:00:27.\n","  Batch   300  of  3,051.    Elapsed: 0:00:41.\n","  Batch   400  of  3,051.    Elapsed: 0:00:55.\n","  Batch   500  of  3,051.    Elapsed: 0:01:08.\n","  Batch   600  of  3,051.    Elapsed: 0:01:22.\n","  Batch   700  of  3,051.    Elapsed: 0:01:36.\n","  Batch   800  of  3,051.    Elapsed: 0:01:50.\n","  Batch   900  of  3,051.    Elapsed: 0:02:03.\n","  Batch 1,000  of  3,051.    Elapsed: 0:02:17.\n","  Batch 1,100  of  3,051.    Elapsed: 0:02:31.\n","  Batch 1,200  of  3,051.    Elapsed: 0:02:44.\n","  Batch 1,300  of  3,051.    Elapsed: 0:02:58.\n","  Batch 1,400  of  3,051.    Elapsed: 0:03:12.\n","  Batch 1,500  of  3,051.    Elapsed: 0:03:26.\n","  Batch 1,600  of  3,051.    Elapsed: 0:03:39.\n","  Batch 1,700  of  3,051.    Elapsed: 0:03:53.\n","  Batch 1,800  of  3,051.    Elapsed: 0:04:07.\n","  Batch 1,900  of  3,051.    Elapsed: 0:04:20.\n","  Batch 2,000  of  3,051.    Elapsed: 0:04:34.\n","  Batch 2,100  of  3,051.    Elapsed: 0:04:48.\n","  Batch 2,200  of  3,051.    Elapsed: 0:05:01.\n","  Batch 2,300  of  3,051.    Elapsed: 0:05:15.\n","  Batch 2,400  of  3,051.    Elapsed: 0:05:29.\n","  Batch 2,500  of  3,051.    Elapsed: 0:05:42.\n","  Batch 2,600  of  3,051.    Elapsed: 0:05:56.\n","  Batch 2,700  of  3,051.    Elapsed: 0:06:10.\n","  Batch 2,800  of  3,051.    Elapsed: 0:06:24.\n","  Batch 2,900  of  3,051.    Elapsed: 0:06:37.\n","  Batch 3,000  of  3,051.    Elapsed: 0:06:51.\n","\n","Accuracy: 0.90\n","Test took: 0:06:58\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U7SzL1IBe1Dm"},"source":["<br>\n","<br>\n","\n","# **새로운 문장 테스트**"]},{"cell_type":"code","metadata":{"id":"Tb4v_VfEfGQB"},"source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 160\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C12NL1Fvgv4E"},"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQezr0tljJlM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608468019896,"user_tz":-540,"elapsed":419150,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"e85d0adf-3e87-421a-a531-d3b2f4f3d849"},"source":["logits = test_sentences(['주연배우가 아깝다. 총체적 난국...'])\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 5.247629 -4.963285]]\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-9MQ0SK0jofN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608468019896,"user_tz":-540,"elapsed":419146,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"d9336634-274c-4656-b539-a0fe5e60170b"},"source":["logits = test_sentences(['최고의 영화'])\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 0.63411057 -1.0021025 ]]\n","0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P7Zuq23cwGYw"},"source":["<br>\n","<br>\n","\n","# **모델 저장 및 로드**"]},{"cell_type":"code","metadata":{"id":"uFD4Tq4CwE2W"},"source":["torch.save(model, 'content/MyDrive/sentiment_analysis/kcBERT_nsmc_10epochs.pt')  # 저장\n","# model = torch.load('./gdrive/MyDrive/sentiment_analysis/bert_nsmc.pt')  # 로드"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCq12RwusHRc"},"source":["## **모델 로드 후 predict**"]},{"cell_type":"code","metadata":{"id":"SW_WlT3bwO40"},"source":["model = torch.load('content/MyDrive/sentiment_analysis/kcBERT_nsmc.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"4Qu6paWx1dqg","executionInfo":{"status":"ok","timestamp":1608468032617,"user_tz":-540,"elapsed":974,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"ccf4d60b-dbdc-4d5b-f2b0-6d5f7cbf32c0"},"source":["# kaggle 경진대회용 파일 변환\n","df = pd.read_csv('content/MyDrive/sentiment_analysis/ko_data.csv', encoding='CP949')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>정말 많이 울었던 영화입니다.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>시간 낭비예요.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>이걸 영화로 만드는 거야?얼마나 가는지 보자.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id                                Sentence\n","0   0                        정말 많이 울었던 영화입니다.\n","1   1                                시간 낭비예요.\n","2   2  포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.\n","3   3    지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!\n","4   4               이걸 영화로 만드는 거야?얼마나 가는지 보자."]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"Rv-vSuBi1u13"},"source":["def sentence_preprocessing(sentence):\n","    # spaced_sent = spacing(sentence)\n","    only_korean_sent = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣!.~;^ ]', '', sentence)\n","\n","    if len(only_korean_sent) == 0:\n","      pass\n","\n","    result = repeat_normalize(only_korean_sent, num_repeats=2)\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywo25MM7Usn5"},"source":["def sent2label(sentence):\n","    logits = test_sentences([sentence])\n","    label = np.argmax(logits)\n","\n","    return label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"r58zvMgm3fJd","executionInfo":{"status":"ok","timestamp":1608468324780,"user_tz":-540,"elapsed":288467,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"56a27bb8-fe46-4377-a853-b69fedc8306b"},"source":["df['Predicted'] = df['Sentence'].apply(sentence_preprocessing)\n","df['Label'] = df['Predicted'].apply(sent2label)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Sentence</th>\n","      <th>Predicted</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>정말 많이 울었던 영화입니다.</td>\n","      <td>정말 많이 울었던 영화입니다.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>시간 낭비예요.</td>\n","      <td>시간 낭비예요.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.</td>\n","      <td>포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!</td>\n","      <td>지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>이걸 영화로 만드는 거야?얼마나 가는지 보자.</td>\n","      <td>이걸 영화로 만드는 거야얼마나 가는지 보자.</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id  ... Label\n","0   0  ...     1\n","1   1  ...     0\n","2   2  ...     1\n","3   3  ...     1\n","4   4  ...     0\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"jSU3xiVC1xj_","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1608468324781,"user_tz":-540,"elapsed":286448,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"20e5fb6a-3936-4f63-d072-a2a722a9e2e8"},"source":["temp_df = df[['Id', 'Label']]\n","temp_df.columns = ['Id', 'Predicted']\n","temp_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id  Predicted\n","0   0          1\n","1   1          0\n","2   2          1\n","3   3          1\n","4   4          0"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"-xILoIcX7Ya9"},"source":["temp_df.to_csv('content/MyDrive/sentiment_analysis/nsmc_sample_20201220.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"vAa23q7sk4p-","executionInfo":{"status":"ok","timestamp":1608468324781,"user_tz":-540,"elapsed":276239,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilkUY9b1jRrtCl8kOXigy9-nLr12Plh51moRKkTA=s64","userId":"13477748337230346832"}},"outputId":"9a46e33d-8051-4ce7-f05d-8ca000029624"},"source":["test_df = pd.read_csv('content/MyDrive/sentiment_analysis/nsmc_sample.csv')\n","test_df.head()\n","# test_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id  Predicted\n","0   0          1\n","1   1          0\n","2   2          1\n","3   3          1\n","4   4          0"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"m3nJC27INl26"},"source":[""],"execution_count":null,"outputs":[]}]}