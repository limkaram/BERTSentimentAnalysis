{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_friends_sentiment_analysis.ipynb","provenance":[{"file_id":"1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP","timestamp":1607162921263},{"file_id":"1eBwayEhGkAp0XyNdqkWy1wx7ZrC85x2l","timestamp":1574838471826},{"file_id":"1-CuJSK8NpSjMFo_kgmYaKkEv7VrlDT3Y","timestamp":1574768751397},{"file_id":"1bK2xoo6te3hKb_hdnkoyLODMUJROv1DX","timestamp":1574765829395}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2d7cc2b2a5304c6db26c700b3e79d83b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0ac2a10257c24179bbd79b30c855a593","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_653e5a206693488c85b48fa3a0e03d88","IPY_MODEL_8b8f3f90429f42a29931597c9725ab20"]}},"0ac2a10257c24179bbd79b30c855a593":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"653e5a206693488c85b48fa3a0e03d88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0780e41fd7124940b53d89f88ebd06a5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8e9b032fdebc4a038740dfb1b3cd95ff"}},"8b8f3f90429f42a29931597c9725ab20":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c7628c5e48f14417bf2860bb0a202ba7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 291kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4a383d77e6614c8c882253c70d1060af"}},"0780e41fd7124940b53d89f88ebd06a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8e9b032fdebc4a038740dfb1b3cd95ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7628c5e48f14417bf2860bb0a202ba7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4a383d77e6614c8c882253c70d1060af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45eed3dd13b04fabae5b49b966a2fccc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_36e85146e49c47859d3be78022fb59a7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dff204ac01c2473abaa38f100b7080d7","IPY_MODEL_cfa96478bcec4bf5a2ae611d50d19c6a"]}},"36e85146e49c47859d3be78022fb59a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dff204ac01c2473abaa38f100b7080d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7976ca72b61b40a5bb6297b774fc88c7","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec35cffdf1bc469193ac4e0d1c5d5efb"}},"cfa96478bcec4bf5a2ae611d50d19c6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f331c8ff8ddb4b32ba62ec9c16ef5892","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:00&lt;00:00, 1.08kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d37e0062939f46ceae8bcc12e512e683"}},"7976ca72b61b40a5bb6297b774fc88c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ec35cffdf1bc469193ac4e0d1c5d5efb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f331c8ff8ddb4b32ba62ec9c16ef5892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d37e0062939f46ceae8bcc12e512e683":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a6df14be4af648b29f5658e818d4fbc8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0196a315ba1841178b4e93bccb2f65d2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1931a13673974b5a98127ee71eb72776","IPY_MODEL_b19aa4d4668540789eb020b2973188de"]}},"0196a315ba1841178b4e93bccb2f65d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1931a13673974b5a98127ee71eb72776":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c61f247e220042428bcb9a4c1e1fef6c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1338740706,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1338740706,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ad6715281b2465fa7c0399b246b472e"}},"b19aa4d4668540789eb020b2973188de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_141b0014666343758a7c36aa27b9e608","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.34G/1.34G [00:17&lt;00:00, 77.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_18148dc9ca5f447eb04e31bde48e2f8e"}},"c61f247e220042428bcb9a4c1e1fef6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ad6715281b2465fa7c0399b246b472e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"141b0014666343758a7c36aa27b9e608":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"18148dc9ca5f447eb04e31bde48e2f8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"i45d7E0L8bZ_"},"source":["<br>\n","<br>\n","\n","# **준비 사항**"]},{"cell_type":"code","metadata":{"id":"WkAHQrj2Vjbl","executionInfo":{"status":"ok","timestamp":1608095172258,"user_tz":-540,"elapsed":2908,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}}},"source":["# Hugging Face의 트랜스포머 모델을 설치\n","!pip install transformers --quiet"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"75dIz2fNWG8F","executionInfo":{"status":"ok","timestamp":1608093734773,"user_tz":-540,"elapsed":945,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}}},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime\n","import json\n","import os\n","import re"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_U3uMySBCIV"},"source":["<br>\n","<br>\n","\n","# **데이터 로드**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u079y7rVOQJe","executionInfo":{"status":"ok","timestamp":1608092291234,"user_tz":-540,"elapsed":26450,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"03e491e0-e6b3-41a6-d0eb-b81b223bd57a"},"source":["from google.colab import drive\n","drive.mount('content')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u7TCV1rIOY-c","executionInfo":{"status":"ok","timestamp":1608094620622,"user_tz":-540,"elapsed":1913,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}}},"source":["# friends dataset 탐색\n","root_dir = \"content\"\n","\n","for (root, dirs, files) in os.walk(root_dir):\n","    for file in files:\n","        if file.lower().startswith('friends') and file.lower().endswith('.json'):\n","            filepath = root\n","            \n","print(filepath)\n","\n","# JSON -> DataFrame\n","data = {'train': {'speaker': [], 'utterance': [], 'emotion': []},\n","        'dev': {'speaker': [], 'utterance': [], 'emotion': []},\n","        'test': {'speaker': [], 'utterance': [], 'emotion': []}}\n","\n","for dtype in ['train', 'dev', 'test']:\n","    for dialog in json.loads(open(os.path.join(filepath, 'friends_' + dtype + '.json')).read()):\n","        for line in dialog:\n","            data[dtype]['speaker'].append(line['speaker'])\n","            data[dtype]['utterance'].append(line['utterance'])\n","            data[dtype]['emotion'].append(line['emotion'])\n","\n","train = pd.DataFrame(data['train'])\n","dev = pd.DataFrame(data['dev'])\n","test = pd.DataFrame(data['test'])"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"id":"WZRll0RaOMAX","executionInfo":{"status":"ok","timestamp":1608094634032,"user_tz":-540,"elapsed":988,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"82c82874-e5ba-49ef-bfe9-1bc062b0ce03"},"source":["print(train.shape)\n","print(dev.shape)\n","print(test.shape)\n","train.head(10)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["(10561, 3)\n","(1178, 3)\n","(2764, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>speaker</th>\n","      <th>utterance</th>\n","      <th>emotion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Mark</td>\n","      <td>Why do all youre coffee mugs have numbers on ...</td>\n","      <td>surprise</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Rachel</td>\n","      <td>Oh. Thats so Monica can keep track. That way ...</td>\n","      <td>non-neutral</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Rachel</td>\n","      <td>Y'know what?</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ross</td>\n","      <td>It didnt.</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Frank</td>\n","      <td>Okay, so what you used to have with Rachel, is...</td>\n","      <td>joy</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Joey</td>\n","      <td>Now, wh-what, what is that like?</td>\n","      <td>surprise</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Frank</td>\n","      <td>Its so cool man, its so, its just cause be...</td>\n","      <td>joy</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Ross</td>\n","      <td>Yeah, yeah.</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Joey</td>\n","      <td>Why cant I find that?</td>\n","      <td>non-neutral</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Ross</td>\n","      <td>Dont ask me, I had it and I blew it!</td>\n","      <td>anger</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  speaker                                          utterance      emotion\n","0    Mark  Why do all youre coffee mugs have numbers on ...     surprise\n","1  Rachel  Oh. Thats so Monica can keep track. That way ...  non-neutral\n","2  Rachel                                       Y'know what?      neutral\n","3    Ross                                         It didnt.      neutral\n","4   Frank  Okay, so what you used to have with Rachel, is...          joy\n","5    Joey                   Now, wh-what, what is that like?     surprise\n","6   Frank  Its so cool man, its so, its just cause be...          joy\n","7    Ross                                        Yeah, yeah.      neutral\n","8    Joey                             Why cant I find that?  non-neutral\n","9    Ross              Dont ask me, I had it and I blew it!        anger"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3gwQst3FYG9O","executionInfo":{"status":"ok","timestamp":1607851849290,"user_tz":-540,"elapsed":1040,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"cb965c3c-8efc-44c1-9f07-828321aeb393"},"source":["#  모든 데이터셋을 통틀어 가장 긴 토큰 길이 확인\n","temp_df = pd.concat([train, dev, test])\n","\n","length = 0\n","max_length = 0\n","\n","for text in temp_df.utterance.values:\n","    if len(text.split()) > length:\n","        max_length = len(text.split())\n","    length = len(text.split())\n","print(max_length)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XgjMzosCDD35"},"source":["<br>\n","<br>\n","\n","# **전처리 - 훈련셋**"]},{"cell_type":"code","metadata":{"id":"2GoESQ0jbybJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851855330,"user_tz":-540,"elapsed":1348,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"a46c89e9-7e46-4176-cb3e-752122b98a56"},"source":["# 리뷰 문장 추출\n","sentences = train['utterance']\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    also I was the point person on my companys tr...\n","1                     You mustve had your hands full.\n","2                              That I did. That I did.\n","3        So lets talk a little bit about your duties.\n","4                               My duties?  All right.\n","5    Now youll be heading a whole division, so you...\n","6                                               I see.\n","7    But therell be perhaps 30 people under you so...\n","8                                        Good to know.\n","9                                We can go into detail\n","Name: utterance, dtype: object"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"8KkJZvhccRUJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851858288,"user_tz":-540,"elapsed":972,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"359281d9-5baa-4a4b-871c-917fe3e72121"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] also I was the point person on my company\\x92s transition from the KL-5 to GR-6 system. [SEP]',\n"," '[CLS] You must\\x92ve had your hands full. [SEP]',\n"," '[CLS] That I did. That I did. [SEP]',\n"," '[CLS] So let\\x92s talk a little bit about your duties. [SEP]',\n"," '[CLS] My duties?  All right. [SEP]',\n"," '[CLS] Now you\\x92ll be heading a whole division, so you\\x92ll have a lot of duties. [SEP]',\n"," '[CLS] I see. [SEP]',\n"," '[CLS] But there\\x92ll be perhaps 30 people under you so you can dump a certain amount on them. [SEP]',\n"," '[CLS] Good to know. [SEP]',\n"," '[CLS] We can go into detail [SEP]']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"7hBblIVQcXJR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851861742,"user_tz":-540,"elapsed":989,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"bf9cffd6-a810-418f-e251-6064a525f569"},"source":["# 라벨 추출\n","encoder = LabelEncoder()\n","labels = train['emotion'].values\n","encoder.fit(labels)\n","labels = encoder.transform(labels)\n","\n","print(encoder.classes_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['anger' 'disgust' 'fear' 'joy' 'neutral' 'non-neutral' 'sadness'\n"," 'surprise']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PwEplfDvcnZG","colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["2d7cc2b2a5304c6db26c700b3e79d83b","0ac2a10257c24179bbd79b30c855a593","653e5a206693488c85b48fa3a0e03d88","8b8f3f90429f42a29931597c9725ab20","0780e41fd7124940b53d89f88ebd06a5","8e9b032fdebc4a038740dfb1b3cd95ff","c7628c5e48f14417bf2860bb0a202ba7","4a383d77e6614c8c882253c70d1060af"]},"executionInfo":{"status":"ok","timestamp":1607851867991,"user_tz":-540,"elapsed":4772,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"a2db809f-476a-4bd5-ee56-7edf8c108bd5"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d7cc2b2a5304c6db26c700b3e79d83b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","[CLS] also I was the point person on my companys transition from the KL-5 to GR-6 system. [SEP]\n","['[CLS]', 'also', 'I', 'was', 'the', 'point', 'person', 'on', 'my', 'company', '##s', 'transition', 'from', 'the', 'K', '##L', '-', '5', 'to', 'G', '##R', '-', '6', 'system', '.', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VJ76KiP_dLn-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851867992,"user_tz":-540,"elapsed":1223,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"365661f4-28d7-496e-c9aa-c83aaa2c9091"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 20\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 101, 1145,  146, 1108, 1103, 1553, 1825, 1113, 1139, 1419, 1116,\n","       6468, 1121, 1103,  148, 2162,  118,  126, 1106,  144])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"pKfL8SotdVaW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851869940,"user_tz":-540,"elapsed":1427,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"956ec6ad-d2f8-4c44-e2e0-a03feb9004ed"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1f5Vq3-7eNKH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851871168,"user_tz":-540,"elapsed":929,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"b5b05ce0-501d-4132-e6ca-ad38c843c4a1"},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, \n","                                                                                    random_state=2018, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n","                                                       input_ids,\n","                                                       random_state=2018, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\t\t\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 101, 1135,  112,  188, 1136, 6276,  106,  102,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n","tensor([  101,  1337, 18243,  1144,  1400,  1106,  1129,  4482,  1174,   117,\n","         3008,   117,  1115,  1116,  1240,  2853,  1208,   119,   102,     0])\n","tensor(4)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I3vlyUJuVRo5"},"source":["# 배치 사이즈\n","batch_size = 64\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkqUHx51dffp"},"source":["<br>\n","<br>\n","\n","# **전처리 - 테스트셋**"]},{"cell_type":"code","metadata":{"id":"xgrsNuArd4pj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851879340,"user_tz":-540,"elapsed":1768,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"ff92bebc-60a4-4e1b-8e19-91b17ed07eee"},"source":["# 리뷰 문장 추출\n","sentences = test['emotion']\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       surprise\n","1    non-neutral\n","2        neutral\n","3        neutral\n","4            joy\n","5       surprise\n","6            joy\n","7        neutral\n","8    non-neutral\n","9          anger\n","Name: emotion, dtype: object"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"Gtz3QZt9d4pz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851879765,"user_tz":-540,"elapsed":1643,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"1cb0b6af-19fe-4684-9ca4-14aaaeb30586"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] surprise [SEP]',\n"," '[CLS] non-neutral [SEP]',\n"," '[CLS] neutral [SEP]',\n"," '[CLS] neutral [SEP]',\n"," '[CLS] joy [SEP]',\n"," '[CLS] surprise [SEP]',\n"," '[CLS] joy [SEP]',\n"," '[CLS] neutral [SEP]',\n"," '[CLS] non-neutral [SEP]',\n"," '[CLS] anger [SEP]']"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"li8oRajbd4p3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851882213,"user_tz":-540,"elapsed":1428,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"07e5bb5b-c94c-40e5-b54f-ad2b9a88e670"},"source":["# 라벨 추출\n","encoder = LabelEncoder()\n","labels = test['emotion'].values\n","encoder.fit(labels)\n","labels = encoder.transform(labels)\n","\n","print(encoder.classes_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['anger' 'disgust' 'fear' 'joy' 'neutral' 'non-neutral' 'sadness'\n"," 'surprise']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lvpQ49nEd4p6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851884223,"user_tz":-540,"elapsed":2637,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"b3eaeefa-5c66-4b3a-a6d4-e9cf0ab32f35"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[CLS] surprise [SEP]\n","['[CLS]', 'surprise', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HI9viuAvd4p_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851885505,"user_tz":-540,"elapsed":1847,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"d2dc127c-cb1d-4bca-fe53-44c0b3e17a25"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 20\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 101, 3774,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"v1NKmP0Fd4qD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851888700,"user_tz":-540,"elapsed":2035,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"23d799d4-d621-4263-ca84-572715dfe1b7"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RIkaYCGbd4qG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851890789,"user_tz":-540,"elapsed":1276,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"fea60bd3-9a06-42e7-b9fe-3c8cf30b72ce"},"source":["# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)\n","\n","print(test_inputs[0])\n","print(test_labels[0])\n","print(test_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 101, 3774,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","tensor(7)\n","tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7gwdYv1Ad4qK"},"source":["# 배치 사이즈\n","batch_size = 64\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBvpU-Hfgcth"},"source":["<br>\n","<br>\n","\n","# **모델 생성**"]},{"cell_type":"code","metadata":{"id":"heToD1ev0mOg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851898997,"user_tz":-540,"elapsed":4553,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"fb1d3546-6e22-4a76-bf6c-808d60a1f0cf"},"source":["# GPU 디바이스 이름 구함\n","device_name = tf.test.gpu_device_name()\n","\n","# GPU 디바이스 이름 검사\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f6enIxvt1FB2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607851898997,"user_tz":-540,"elapsed":4192,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"c6372df6-543c-4902-d2b0-95819c672b04"},"source":["# 디바이스 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MS2MXSiLg5zC","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["45eed3dd13b04fabae5b49b966a2fccc","36e85146e49c47859d3be78022fb59a7","dff204ac01c2473abaa38f100b7080d7","cfa96478bcec4bf5a2ae611d50d19c6a","7976ca72b61b40a5bb6297b774fc88c7","ec35cffdf1bc469193ac4e0d1c5d5efb","f331c8ff8ddb4b32ba62ec9c16ef5892","d37e0062939f46ceae8bcc12e512e683","a6df14be4af648b29f5658e818d4fbc8","0196a315ba1841178b4e93bccb2f65d2","1931a13673974b5a98127ee71eb72776","b19aa4d4668540789eb020b2973188de","c61f247e220042428bcb9a4c1e1fef6c","5ad6715281b2465fa7c0399b246b472e","141b0014666343758a7c36aa27b9e608","18148dc9ca5f447eb04e31bde48e2f8e"]},"executionInfo":{"status":"ok","timestamp":1607851941616,"user_tz":-540,"elapsed":38747,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"b915e07c-4ae6-4e22-e53a-465eb98d2870"},"source":["# 분류를 위한 BERT 모델 생성\n","model = BertForSequenceClassification.from_pretrained(\"bert-large-cased\", num_labels=8)\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45eed3dd13b04fabae5b49b966a2fccc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6df14be4af648b29f5658e818d4fbc8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1338740706.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n","      (position_embeddings): Embedding(512, 1024)\n","      (token_type_embeddings): Embedding(2, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"ZIdfbLTuWmxk"},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 5\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzCHV_ghj7DM"},"source":["<br>\n","<br>\n","\n","# **모델 학습**"]},{"cell_type":"code","metadata":{"id":"S0-p6pPVXCRe"},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJXISnJzCdLM"},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muU2kS2GCh4y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607859386828,"user_tz":-540,"elapsed":405945,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"6bb7042e-d8f7-46fd-cbfc-d51f2287f13a"},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 5 ========\n","Training...\n","\n","  Average training loss: 0.10\n","  Training epcoh took: 0:01:19\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation took: 0:00:02\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","\n","  Average training loss: 0.10\n","  Training epcoh took: 0:01:19\n","\n","Running Validation...\n","  Accuracy: 0.59\n","  Validation took: 0:00:02\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","\n","  Average training loss: 0.09\n","  Training epcoh took: 0:01:19\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation took: 0:00:02\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","\n","  Average training loss: 0.08\n","  Training epcoh took: 0:01:19\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation took: 0:00:02\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","\n","  Average training loss: 0.07\n","  Training epcoh took: 0:01:19\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation took: 0:00:02\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hxlXEBA0WefL"},"source":["에폭마다 훈련셋과 검증셋을 반복하여 학습을 수행합니다. "]},{"cell_type":"markdown","metadata":{"id":"6BVbl4Zjatzn"},"source":["<br>\n","<br>\n","\n","# **테스트셋 평가**"]},{"cell_type":"code","metadata":{"id":"c5KHb6RkbHdj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607859392675,"user_tz":-540,"elapsed":410319,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"3f7dd4d0-e19a-45c1-cef7-a4ac0e516b64"},"source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Accuracy: 0.72\n","Test took: 0:00:06\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U7SzL1IBe1Dm"},"source":["<br>\n","<br>\n","\n","# **새로운 문장 테스트**"]},{"cell_type":"code","metadata":{"id":"Tb4v_VfEfGQB"},"source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 128\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C12NL1Fvgv4E"},"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQezr0tljJlM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607860026602,"user_tz":-540,"elapsed":1394,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"5c624c40-fce0-4963-a61c-2da5711ee1d6"},"source":["def predict(sentence):\n","    logits = test_sentences([sentence])\n","    label = np.argmax(logits)\n","    emotion = encoder.classes_[label]\n","    return emotion\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-1.5722619  3.2715435 -2.461211  -2.2554595 -2.7080073  4.1809144\n","  -2.2216432  5.141793 ]]\n","7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-9MQ0SK0jofN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607859392677,"user_tz":-540,"elapsed":408558,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"220af2a3-3c3c-420b-8bef-73b25da8448a"},"source":["logits = test_sentences([\"i'm sorry to hear that\"])\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-2.777257  -1.3429648 -1.6831119 -1.0271511  0.8114235 -0.9242955\n","   9.615311  -2.1560647]]\n","6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cTaav_opclJU"},"source":["torch.save(model, filepath)  # 저장\n","# model = torch.load('./gdrive/MyDrive/sentiment_analysis/bert_nsmc.pt')  # 로드"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvk7eeiieUAG","executionInfo":{"status":"ok","timestamp":1608095247827,"user_tz":-540,"elapsed":977,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"c0f0bb54-4e3e-4725-ef9a-5714f6584252"},"source":["os.listdir(filepath)"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ko_data.csv',\n"," 'koELECTRA_nsmc_v2.pt',\n"," 'koELECTRA_nsmc_v3.pt',\n"," 'ELECTRA_friends_multi_classification.ipynb',\n"," 'koELECTRA_nsmc_v4.pt',\n"," 'sample.csv',\n"," 'en_data.csv',\n"," 'BERT_friends.pt',\n"," 'friends_sample.csv',\n"," 'koELECTRA_nsmc_v5.pt',\n"," 'koELECTRA_nsmc_10epochs.pt',\n"," 'nsmc_sample.csv',\n"," 'KoELECTRA_nsmc_sentiment_analysis.ipynb',\n"," 'BERT_friends_sentiment_analysis.ipynb',\n"," 'EmotionLines_friends_annotation.tar.gz',\n"," 'friends_dev.json',\n"," 'friends_test.json',\n"," 'friends_train.json']"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"Y_FNdfqhwIXS"},"source":["</br>\n","</br>\n","\n","### kaggle 경진대회 dataset 활용 예측"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekfry363ehnW","executionInfo":{"status":"ok","timestamp":1607859399530,"user_tz":-540,"elapsed":410857,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"4b815311-e993-4f73-ae33-a9ba34bd3c9b"},"source":["# 케글 predict 평가용 데이터 로드\n","kaggle_dataset = pd.read_csv(os.path.join(filepath, 'en_data.csv'))\n","kaggle_dataset.head()\n","len(kaggle_dataset)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1623"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"id":"1w-1MmmTfvqn"},"source":["# 문장을 예측한 emotion으로 변환\n","def predict(sentence):\n","    logits = test_sentences([sentence])\n","    label = np.argmax(logits)\n","    emotion = encoder.classes_[label]\n","    return emotion"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"aQLVb9WRi1Qd","executionInfo":{"status":"ok","timestamp":1607859438814,"user_tz":-540,"elapsed":439065,"user":{"displayName":"임가람","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ2VJo13lEkkqZ9s4l8PO7yaWAGHh2_Zp1BAOLTg=s64","userId":"13477748337230346832"}},"outputId":"5cc774c9-d070-4189-b166-38f98ffe415e"},"source":["kaggle_dataset['Predicted'] = kaggle_dataset['utterance'].apply(predict)\n","kaggle_dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>i_dialog</th>\n","      <th>i_utterance</th>\n","      <th>speaker</th>\n","      <th>utterance</th>\n","      <th>Predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Phoebe</td>\n","      <td>Alright, whadyou do with him?</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Monica</td>\n","      <td>Oh! You're awake!</td>\n","      <td>surprise</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Joey</td>\n","      <td>Then you gotta come clean with Ma! This is not...</td>\n","      <td>non-neutral</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Mr. Tribbiani</td>\n","      <td>Yeah, but this is</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>Joey</td>\n","      <td>I don't wanna hear it! Now go to my room!</td>\n","      <td>disgust</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  i_dialog  ...                                          utterance    Predicted\n","0   0         0  ...                      Alright, whadyou do with him?      neutral\n","1   1         0  ...                                  Oh! You're awake!     surprise\n","2   2         0  ...  Then you gotta come clean with Ma! This is not...  non-neutral\n","3   3         0  ...                                  Yeah, but this is      neutral\n","4   4         0  ...          I don't wanna hear it! Now go to my room!      disgust\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"code","metadata":{"id":"ZD17Bt8xi9S_"},"source":["temp_df = kaggle_dataset[['id', 'Predicted']]\n","temp_df = temp_df.rename(columns={'id': 'Id'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FuXF5Cp_jqV6"},"source":["temp_df.to_csv('content/MyDrive/sentiment_analysis/friends_sample.csv', index=False)"],"execution_count":null,"outputs":[]}]}